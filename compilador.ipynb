{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8g7v2sVbvqj"
      },
      "source": [
        "<h3>Token</h3>\n",
        "<h4>Analisador Lexico - AFD </h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "txG3FR2ubvql"
      },
      "outputs": [],
      "source": [
        "T_KEYWORD = \"<keyword %s>\"\n",
        "T_OP = \"<op %s>\"\n",
        "T_INT = \"<int %s>\"\n",
        "T_STRING = \"<string %s>\"\n",
        "T_IDENTIF = \"<id %s>\"\n",
        "T_SPECIAL = \"<special %s>\"\n",
        "T_PUNCT = \"<punct %s>\"\n",
        "T_DOT = \"<dot>\"\n",
        "T_CONDITIONAL_OP = \"<conditional_op %s>\"\n",
        "T_COMMENT = \"<comment>\"\n",
        "\n",
        "class Token():\n",
        "    def __init__(self, tipo, valor=None):\n",
        "        self.tipo = tipo\n",
        "        self.valor = valor\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Token(tipo={self.tipo}, valor={self.valor})\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass\n",
        "\n",
        "def tokenize_line(line, line_number):\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    n = len(line)\n",
        "\n",
        "    while i < n:\n",
        "        if line[i].isspace():\n",
        "            i += 1\n",
        "        elif line[i] == '#':  # Comment\n",
        "            while i < n:\n",
        "                i += 1\n",
        "            break\n",
        "            # start = i\n",
        "            # while i < n:\n",
        "            #     i += 1\n",
        "            # tokens.append(Token(\"T_COMMENT\", line[start:i]))\n",
        "            break\n",
        "        elif line[i].isalpha():\n",
        "            start = i\n",
        "            while i < n and (line[i].isalnum() or line[i] == '_'):\n",
        "                i += 1\n",
        "            word = line[start:i]\n",
        "            if word in [\"var\", \"func\", \"if\", \"elif\", \"else\", \"return\", \"object\", \"init\"]:\n",
        "                tokens.append(Token(\"T_KEYWORD\", word))\n",
        "            elif word in [\"true\", \"false\", \"null\", \"end\", \"main\"]:\n",
        "                tokens.append(Token(\"T_SPECIAL\", word))\n",
        "            else:\n",
        "                tokens.append(Token(\"T_IDENTIF\", word))\n",
        "        elif line[i].isdigit():\n",
        "            start = i\n",
        "            while i < n and line[i].isdigit():\n",
        "                i += 1\n",
        "            tokens.append(Token(\"T_INT\", line[start:i]))\n",
        "        elif line[i] == '\"':\n",
        "            start = i\n",
        "            i += 1\n",
        "            while i < n and line[i] != '\"':\n",
        "                i += 1\n",
        "            if i >= n:\n",
        "                print(f\"Erro: String não fechada na linha {line_number}\")\n",
        "                raise StopExecution\n",
        "            i += 1\n",
        "            tokens.append(Token(\"T_STRING\", line[start:i]))\n",
        "        # elif line[i] in \"=<>!+-*/\":\n",
        "        #     start = i\n",
        "        #     i += 1\n",
        "        #     if i < n and line[i] == \"=\":\n",
        "        #         i += 1\n",
        "        #     tokens.append(Token(\"T_OP\", line[start:i]))\n",
        "        elif line[i] in \"=<>!\":\n",
        "            start = i\n",
        "            i += 1\n",
        "            if i < n and line[i] == \"=\":\n",
        "                i += 1\n",
        "                tokens.append(Token(\"T_OP\", line[start:i]))\n",
        "            else:\n",
        "                tokens.append(Token(\"T_OP\", line[start:i]))\n",
        "        elif line[i] in \"+-*/\":\n",
        "            tokens.append(Token(\"T_OP\", line[i]))\n",
        "            i += 1\n",
        "        elif line[i] in \"(),[]{}\":\n",
        "            tokens.append(Token(\"T_PUNCT\", line[i]))\n",
        "            i += 1\n",
        "        elif line[i] in \"?:\":\n",
        "            tokens.append(Token(\"T_CONDITIONAL_OP\", line[i]))\n",
        "            i += 1\n",
        "        elif line[i] == '.':\n",
        "            tokens.append(Token(\"T_DOT\"))\n",
        "            i += 1\n",
        "        else:\n",
        "            print(f\"Erro: Caractere não reconhecido '{line[i]}' na linha {line_number}\")\n",
        "            raise StopExecution\n",
        "    return tokens\n",
        "\n",
        "def tokenize():\n",
        "\n",
        "    try:\n",
        "        token_total = [];\n",
        "        with open('codigo2.x', 'r') as f:\n",
        "            lines = f.read().splitlines()\n",
        "\n",
        "        line_number = 0\n",
        "        for line in lines:\n",
        "            line_number += 1\n",
        "            tokens = tokenize_line(line, line_number)\n",
        "            token_total.extend(tokens)\n",
        "            #print(tokens)\n",
        "\n",
        "        return token_total\n",
        "\n",
        "    except StopExecution:\n",
        "        print(\"Execução parada devido a erro.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNtC_4P-bvqn"
      },
      "source": [
        "<h3>Analisador Sinatico - Parser </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "aetJ3j6ebvqn"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "    def __init__(self, tokens):\n",
        "        self.tokens = tokens\n",
        "        self.current_idx = 0\n",
        "        self.current_token = self.tokens[0] if tokens else None\n",
        "        self.symbol_table = {}\n",
        "\n",
        "    def get_next_token(self):\n",
        "        self.current_idx += 1\n",
        "        if self.current_idx < len(self.tokens):\n",
        "            self.current_token = self.tokens[self.current_idx]\n",
        "        else:\n",
        "            self.current_token = None\n",
        "\n",
        "    def look_ahead(self):\n",
        "        if self.current_idx + 1 < len(self.tokens):\n",
        "            return self.tokens[self.current_idx + 1]\n",
        "        return None\n",
        "\n",
        "    def eat(self, token_type, specific_value=None):\n",
        "        if self.current_token.tipo == token_type and (specific_value is None or self.current_token.valor == specific_value):\n",
        "            self.get_next_token()\n",
        "        else:\n",
        "            raise Exception(f\"Erro de sintaxe. Esperado: {token_type} - Valor: {specific_value if specific_value else ''}. Recebido: {self.current_token.tipo} - Valor: {self.current_token.valor}\")\n",
        "\n",
        "    def factor(self):\n",
        "        token = self.current_token\n",
        "        if token.tipo == 'T_INT':\n",
        "            self.eat('T_INT')\n",
        "            return int(token.valor)\n",
        "        elif token.tipo == 'T_IDENTIF':\n",
        "            self.eat('T_IDENTIF')\n",
        "            if token.valor not in self.symbol_table:\n",
        "                raise Exception(f\"Variável {token.valor} não definida.\")\n",
        "            return self.symbol_table.get(token.valor, None)\n",
        "        elif token.tipo == 'T_STRING':\n",
        "            self.eat('T_STRING')\n",
        "            return token.valor[1:-1]\n",
        "        elif token.tipo == 'T_PUNCT' and token.valor == '(':\n",
        "            self.eat('T_PUNCT', '(')\n",
        "            result = self.expr()\n",
        "            self.eat('T_PUNCT', ')')\n",
        "            return result\n",
        "        elif token.tipo == \"T_SPECIAL\" and token.valor in [\"true\", \"false\"]:\n",
        "            self.eat(\"T_SPECIAL\")\n",
        "            return token.valor == \"true\"\n",
        "        elif token.tipo == 'T_OP' and token.valor == '?':\n",
        "            return self.ternary_expr()\n",
        "        else:\n",
        "            raise Exception(f\"Erro de sintaxe. Token inesperado: {token.tipo} - Valor: {token.valor}\")\n",
        "\n",
        "    def term(self):\n",
        "        result = self.factor()\n",
        "        while self.current_token is not None and self.current_token.tipo == 'T_OP' and self.current_token.valor in ['*', '/']:\n",
        "            token = self.current_token\n",
        "            if token.valor == '*':\n",
        "                self.eat('T_OP')\n",
        "                result *= self.factor()\n",
        "            elif token.valor == '/':\n",
        "                self.eat('T_OP')\n",
        "                result /= self.factor()\n",
        "        return result\n",
        "\n",
        "    def expr(self):\n",
        "        if '?' in [token.valor for token in self.tokens[self.current_idx:]]:\n",
        "            return self.ternary_expr()\n",
        "        return self.simple_expr()\n",
        "\n",
        "    def simple_expr(self):\n",
        "        result = self.term()\n",
        "        while self.current_token is not None and self.current_token.tipo == 'T_OP' and self.current_token.valor in ['+', '-']:\n",
        "            token = self.current_token\n",
        "            if token.valor == '+':\n",
        "                self.eat('T_OP')\n",
        "                result += self.term()\n",
        "            elif token.valor == '-':\n",
        "                self.eat('T_OP')\n",
        "                result -= self.term()\n",
        "        return result\n",
        "\n",
        "    def assignment(self):\n",
        "        if self.look_ahead() and self.look_ahead().valor == '=':\n",
        "            var_name = self.current_token.valor\n",
        "            self.eat('T_IDENTIF')\n",
        "            self.eat('T_OP', '=')\n",
        "            var_value = self.expr()\n",
        "            self.symbol_table[var_name] = var_value\n",
        "        elif self.look_ahead() and self.look_ahead().valor == '(':\n",
        "            self.call_function()\n",
        "        elif self.look_ahead() and self.look_ahead().valor == '==':\n",
        "            self.relational_expr()\n",
        "        else:\n",
        "            raise Exception(f\"Erro de sintaxe ao tentar atribuir ou chamar função: {self.current_token.valor}\")\n",
        "\n",
        "    def block(self):\n",
        "        self.eat('T_PUNCT', '{')\n",
        "        while self.current_token and (self.current_token.tipo != 'T_PUNCT' or self.current_token.valor != '}'):\n",
        "            self.statement()\n",
        "        self.eat('T_PUNCT', '}')\n",
        "\n",
        "    def conditional_statement(self):\n",
        "        if self.current_token.valor == \"if\":\n",
        "            self.eat('T_KEYWORD')\n",
        "            self.eat('T_PUNCT', '(')\n",
        "            condition = self.relational_expr()\n",
        "            self.eat('T_PUNCT', ')')\n",
        "            self.block()\n",
        "        elif self.current_token.valor == \"elif\":\n",
        "            self.eat('T_KEYWORD')\n",
        "            self.eat('T_PUNCT', '(')\n",
        "            condition = self.relational_expr()\n",
        "            self.eat('T_PUNCT', ')')\n",
        "            self.block()\n",
        "        else:\n",
        "            self.eat('T_KEYWORD')\n",
        "            self.block()\n",
        "\n",
        "    def return_statement(self):\n",
        "        self.eat('T_KEYWORD')  # Consume 'return'\n",
        "        return_value = self.expr()\n",
        "        # Handle or store return value as needed\n",
        "\n",
        "    def ternary_expr(self):\n",
        "        condition = self.relational_expr()\n",
        "        self.eat('T_OP', '?')\n",
        "        true_expr = self.simple_expr()\n",
        "        self.eat('T_OP', ':')\n",
        "        false_expr = self.simple_expr()\n",
        "        return true_expr if condition else false_expr\n",
        "\n",
        "    def loop_statement(self):\n",
        "        self.eat('T_KEYWORD')  # Consumir \"while\"\n",
        "        self.eat('T_PUNCT', '(')\n",
        "        condition = self.relational_expr()\n",
        "        self.eat('T_PUNCT', ')')\n",
        "        while condition:\n",
        "            self.block()\n",
        "            condition = self.relational_expr()\n",
        "\n",
        "    def function_definition(self):\n",
        "        self.eat('T_KEYWORD')  # Consume \"func\"\n",
        "        func_name = self.current_token.valor\n",
        "        self.eat('T_IDENTIF')\n",
        "        self.eat('T_PUNCT', '(')\n",
        "\n",
        "        parameters = []\n",
        "\n",
        "        while self.current_token.tipo == 'T_IDENTIF':\n",
        "          param_name = self.current_token.valor\n",
        "          parameters.append(param_name)\n",
        "          self.eat('T_IDENTIF')\n",
        "\n",
        "          # Se houver mais parâmetros, coma a vírgula\n",
        "          if self.current_token.tipo == 'T_PUNCT' and self.current_token.valor == ',':\n",
        "              self.eat('T_PUNCT', ',')\n",
        "        # Handle function arguments as needed\n",
        "        self.eat('T_PUNCT', ')')\n",
        "        self.block()\n",
        "\n",
        "        self.symbol_table[func_name] = {'parameters': parameters}\n",
        "\n",
        "    def var_declaration(self):\n",
        "        self.eat('T_KEYWORD', 'var')\n",
        "        var_name = self.current_token.valor\n",
        "        self.eat('T_IDENTIF')\n",
        "        self.eat('T_OP', '=')\n",
        "        var_value = self.expr()\n",
        "        self.symbol_table[var_name] = var_value\n",
        "\n",
        "    def call_function(self):\n",
        "        func_name = self.current_token.valor\n",
        "        self.eat('T_IDENTIF')\n",
        "        self.eat('T_PUNCT', '(')\n",
        "\n",
        "        arguments = []\n",
        "\n",
        "        while self.current_token.tipo != 'T_PUNCT' or self.current_token.valor != ')':\n",
        "          arg_value = self.expr()\n",
        "          arguments.append(arg_value)\n",
        "\n",
        "          # Se houver mais argumentos, coma a vírgula\n",
        "          if self.current_token.tipo == 'T_PUNCT' and self.current_token.valor == ',':\n",
        "              self.eat('T_PUNCT', ',')\n",
        "\n",
        "        # Handle function arguments as needed\n",
        "        self.eat('T_PUNCT', ')')\n",
        "\n",
        "        if func_name in self.symbol_table:\n",
        "          parameters = self.symbol_table[func_name]['parameters']\n",
        "          if len(parameters) != len(arguments):\n",
        "              raise Exception(\"Número incorreto de argumentos para a função\")\n",
        "\n",
        "\n",
        "    def relational_expr(self):\n",
        "        left = self.simple_expr()\n",
        "        if self.current_token.valor == '>':\n",
        "            self.eat('T_OP')\n",
        "            right = self.simple_expr()\n",
        "            return left > right\n",
        "        elif self.current_token.valor == '<':\n",
        "            self.eat('T_OP')\n",
        "            right = self.simple_expr()\n",
        "            return left < right\n",
        "        elif self.current_token.valor == '==':\n",
        "            self.eat('T_OP', '==')\n",
        "            right = self.simple_expr()\n",
        "            return left == right\n",
        "        # Add more relational operators as needed\n",
        "\n",
        "    def var_assignment(self):\n",
        "        self.eat('T_KEYWORD', 'var')\n",
        "        var_name = self.current_token.valor\n",
        "        self.eat('T_IDENTIF')\n",
        "        self.eat('T_OP', '=')\n",
        "        var_value = self.expr()\n",
        "        self.symbol_table[var_name] = var_value\n",
        "\n",
        "    def statement(self):\n",
        "      if self.current_token.valor == \"var\":\n",
        "          self.var_declaration()\n",
        "      elif self.current_token.valor in [\"if\", \"elif\", \"else\"]:\n",
        "          self.conditional_statement()\n",
        "      elif self.current_token.valor == \"while\":\n",
        "          self.loop_statement()\n",
        "      elif self.current_token.valor == \"func\":\n",
        "          self.function_definition()\n",
        "      elif self.current_token.tipo == 'T_IDENTIF':\n",
        "          if self.look_ahead() and self.look_ahead().valor == '=':\n",
        "              self.assignment()\n",
        "          else:\n",
        "              # Trate a expressão como uma chamada de função\n",
        "              self.call_function()\n",
        "      elif self.current_token.valor == \"return\":\n",
        "          self.return_statement()\n",
        "      else:\n",
        "          raise Exception(\"Erro de sintaxe: Instrução desconhecida\")\n",
        "\n",
        "    def parse(self):\n",
        "        while self.current_token is not None:\n",
        "          self.statement()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a2UBEDYbvqo"
      },
      "source": [
        "<h3>Compilador</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIhfOaOqbvqo",
        "outputId": "ef9ae575-483d-4b66-d197-922a4b9d65e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lendo o arquivo codigo2.x ...\n",
            "# ANT\n",
            "\n",
            "func home(){\n",
            "    var a = 1\n",
            "    var b = 2\n",
            "    var c = 3\n",
            "\n",
            "    if(c>a){\n",
            "        print(c,\" maior que \", a)\n",
            "    }\n",
            "    elif(c<b){\n",
            "        print(c,\" menor que \", b)\n",
            "    }\n",
            "    else{\n",
            "        print(\"error\")\n",
            "    }\n",
            "\n",
            "    var d = add(a,b)\n",
            "\n",
            "    var boolVar = ternary(a,b)\n",
            "\n",
            "}\n",
            "\n",
            "func add(a,b){\n",
            "    return a + b\n",
            "}\n",
            "\n",
            "func ternary(a,b){\n",
            "    return a == b ? true : false\n",
            "}\n",
            "\n",
            "Tokenização:\n",
            "Tokens:\n",
            "\n",
            "Análise Sintática:\n",
            "Erro no parser: Erro de sintaxe. Esperado: T_OP - Valor: ?. Recebido: T_KEYWORD - Valor: var\n",
            "\n",
            "Estados salvos: {'tokens': [Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_IDENTIF, valor=home), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_OP, valor==), Token(tipo=T_INT, valor=1), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_OP, valor==), Token(tipo=T_INT, valor=2), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor==), Token(tipo=T_INT, valor=3), Token(tipo=T_KEYWORD, valor=if), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor=>), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_STRING, valor=\" maior que \"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=elif), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_OP, valor=<), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=c), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_STRING, valor=\" menor que \"), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=else), Token(tipo=T_PUNCT, valor={), Token(tipo=T_IDENTIF, valor=print), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_STRING, valor=\"error\"), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=d), Token(tipo=T_OP, valor==), Token(tipo=T_IDENTIF, valor=add), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_KEYWORD, valor=var), Token(tipo=T_IDENTIF, valor=boolVar), Token(tipo=T_OP, valor==), Token(tipo=T_IDENTIF, valor=ternary), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_IDENTIF, valor=add), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=return), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_OP, valor=+), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=}), Token(tipo=T_KEYWORD, valor=func), Token(tipo=T_IDENTIF, valor=ternary), Token(tipo=T_PUNCT, valor=(), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_PUNCT, valor=,), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_PUNCT, valor=)), Token(tipo=T_PUNCT, valor={), Token(tipo=T_KEYWORD, valor=return), Token(tipo=T_IDENTIF, valor=a), Token(tipo=T_OP, valor===), Token(tipo=T_IDENTIF, valor=b), Token(tipo=T_CONDITIONAL_OP, valor=?), Token(tipo=T_SPECIAL, valor=true), Token(tipo=T_CONDITIONAL_OP, valor=:), Token(tipo=T_SPECIAL, valor=false), Token(tipo=T_PUNCT, valor=})], 'symbol_table': {}}\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    filename = 'codigo2.x'\n",
        "    print(f\"Lendo o arquivo {filename} ...\")\n",
        "\n",
        "    arquivo = open(filename)\n",
        "    for l in arquivo.readlines():\n",
        "        l = l.replace('\\n','') # remove a quebra de linha\n",
        "        print(l)\n",
        "\n",
        "    # Tokenização\n",
        "    print(\"\\nTokenização:\")\n",
        "    tokens = tokenize()\n",
        "    print(\"Tokens:\")\n",
        "    #for i, token in enumerate(tokens):\n",
        "        #print(f\"{i+1:03}. {token}\")\n",
        "\n",
        "    # Análise Sintática (Parser)\n",
        "    print(\"\\nAnálise Sintática:\")\n",
        "    parser = Parser(tokens)\n",
        "    try:\n",
        "        parser.parse()  # Use the parse method instead of start\n",
        "        print(\"Análise sintática concluída com sucesso!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro no parser: {e}\")\n",
        "\n",
        "    states = {\n",
        "        'tokens': tokens,\n",
        "        'symbol_table': parser.symbol_table\n",
        "    }\n",
        "    print(\"\\nEstados salvos:\", states)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}